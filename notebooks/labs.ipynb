{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bc02b65",
   "metadata": {},
   "source": [
    "# Temporal Linear Network (TLN) based VWAP execution algorithm\n",
    "\n",
    "Python script for training and evaluating a TLN based model to optimize VWAP execution on META 1 minute stock data with market impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76e8a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import keras\n",
    "from tln import TLN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa054550",
   "metadata": {},
   "source": [
    "LOOKBACK: the number of minutes the model looks at to make predictions\n",
    "\n",
    "HORIZON: number of future minutes for which the model predicts the allocation schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eb506a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "LOOKBACK = 120\n",
    "HORIZON = 12\n",
    "TOTAL_SHARES = 1_000_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc2a1d1",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7100a59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "df = pd.read_csv('../datasets/META_1min_firstratedata.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['avg_price'] = (df['high'] + df['low']) / 2\n",
    "df.set_index('timestamp', inplace=True)\n",
    "df = df.between_time('09:30', '16:00').copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b6e118",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "These are the features to be used to train the model. This is based on the paper's recommended insights and uses only past data, ensuring no forward looking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb91e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced feature engineering (no forward-looking)\n",
    "df['returns'] = df['avg_price'].pct_change().fillna(0)\n",
    "df['log_returns'] = np.log(df['avg_price'] / df['avg_price'].shift(1)).fillna(0)\n",
    "df['vol_5'] = df['returns'].rolling(5).std().fillna(0)\n",
    "df['vol_30'] = df['returns'].rolling(30).std().fillna(0)\n",
    "df['vol_scaled'] = df['volume'] / df['volume'].rolling(390*14, min_periods=390).mean().fillna(1)\n",
    "df['hour'] = df.index.hour\n",
    "df['minute'] = df.index.minute\n",
    "df['dow'] = df.index.dayofweek\n",
    "mins_from_open = (df.index.hour - 9) * 60 + (df.index.minute - 30)\n",
    "df['intraday_pos'] = np.clip(mins_from_open / (6.5*60), 0, 1)\n",
    "df['u_shape'] = 1 / (1 + 4 * (df['intraday_pos'] - 0.5)**2)\n",
    "\n",
    "FEATURES = ['avg_price', 'volume', 'vol_scaled', 'log_returns', 'vol_5', 'vol_30',\n",
    "            'hour', 'minute', 'dow', 'intraday_pos', 'u_shape']\n",
    "NUM_FEATURES = len(FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9678c7fc",
   "metadata": {},
   "source": [
    "### Data Splitting into training (first 75%) and testing (last 25%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451f393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data chronologically\n",
    "dates = df.index.normalize().unique()\n",
    "split = int(len(dates) * 0.75)\n",
    "train_dates = dates[:split]\n",
    "test_dates = dates[split:]\n",
    "\n",
    "train_df = df[df.index.normalize().isin(train_dates)].copy()\n",
    "test_df = df[df.index.normalize().isin(test_dates)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fa1b25",
   "metadata": {},
   "source": [
    "### Sequence Creation\n",
    "\n",
    "Defines a function to generate input sequences (X) and targets (Y) for the model. It groups data by date, checks for sufficient length, and creates sliding windows of features and price/volume targets. This structure ensures sequences are contained within single trading days, avoiding cross-day bias and preparing data for time-series modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd0965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence creation (daily grouping, no leak)\n",
    "def make_sequences(data, lookback, horizon):\n",
    "    X, Y = [], []\n",
    "    for _, day in data.groupby(data.index.date):\n",
    "        if len(day) < lookback + horizon: continue\n",
    "        feat = day[FEATURES].values\n",
    "        tgt = day[['avg_price', 'volume']].values\n",
    "        for i in range(len(day) - lookback - horizon + 1):\n",
    "            X.append(feat[i:i+lookback])\n",
    "            Y.append(tgt[i+lookback:i+lookback+horizon])\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2490b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = make_sequences(train_df, LOOKBACK, HORIZON)\n",
    "X_test, y_test = make_sequences(test_df, LOOKBACK, HORIZON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16150a59",
   "metadata": {},
   "source": [
    "### Scale features\n",
    "\n",
    "This fits StandardScalers on the training data for each feature and transforms both train and test sets. Scaling normalizes the features to have mean 0 and standard deviation 1, which helps the model learn more effectively. Importantly, scalers are fit only on training data to prevent information leakage from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2102d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features (fit on train only)\n",
    "scalers = {f: StandardScaler().fit(X_train[:,:,j]) for j, f in enumerate(FEATURES)}\n",
    "for j, f in enumerate(FEATURES):\n",
    "    X_train[:,:,j] = scalers[f].transform(X_train[:,:,j])\n",
    "    X_test[:,:,j] = scalers[f].transform(X_test[:,:,j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5048bac4",
   "metadata": {},
   "source": [
    "### Custom Loss Function\n",
    "\n",
    "The TLN architecture processes temporal sequences of market data to generate optimal allocation schedules, using a custom loss function that directly minimizes the quadratic difference between model VWAP and benchmark VWAP. This is the core feature of this model.\n",
    "\n",
    "Building upon the original VWAP custom loss function, I implemented an enhanced version incorporating smoothness regularization:\n",
    "\n",
    "* Loss = α × VWAP_slippage² + β × smoothness_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5ad6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced loss\n",
    "def vwap_smooth_loss(y_true, y_pred):\n",
    "    price = y_true[:,:,0]\n",
    "    vol = y_true[:,:,1]\n",
    "\n",
    "    # Use TensorFlow operations instead of NumPy\n",
    "    m_val = tf.reduce_sum(y_pred * price, axis=1)\n",
    "    m_sh = tf.reduce_sum(y_pred, axis=1)\n",
    "    m_vwap = m_val / (m_sh + 1e-8)\n",
    "    \n",
    "    b_vwap = tf.reduce_sum(vol * price, axis=1) / (tf.reduce_sum(vol, axis=1) + 1e-8)\n",
    "    \n",
    "    sl = tf.square(m_vwap - b_vwap)\n",
    "    smooth = tf.reduce_mean(tf.square(y_pred[:,1:] - y_pred[:,:-1]), axis=1)\n",
    "    \n",
    "    return sl + 0.05 * smooth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc1b909",
   "metadata": {},
   "source": [
    "### Model Creation\n",
    "\n",
    "Builds the TLN model architecture, starting with an input layer, followed by the TLN layer, dropout for regularization, flattening, a dense layer with L2 regularization, batch normalization, and a softmax output. This setup creates a network that processes sequential data to output allocation probabilities, with built-in mechanisms to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ea36d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model creation\n",
    "inputs = keras.Input(shape=(LOOKBACK, NUM_FEATURES))\n",
    "x = TLN(output_len=HORIZON, output_features=1, hidden_layers=3, use_convolution=True)(inputs)\n",
    "x = keras.layers.Dropout(0.1)(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dense(24, activation='relu', kernel_regularizer=keras.regularizers.l2(1e-3))(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "outputs = keras.layers.Dense(HORIZON, activation='softmax')(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(clipnorm=1.0), loss=vwap_smooth_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54bc3a3",
   "metadata": {},
   "source": [
    "### Training with callbacks\n",
    "\n",
    "Sets up training callbacks for early stopping, learning rate reduction, and scheduling, then fits the model on training data with validation split. The callbacks monitor validation loss to optimize training duration and learning rate dynamically. This improves model convergence and generalization without manual intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897e6635",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5),\n",
    "    keras.callbacks.LearningRateScheduler(lambda e: 0.001 * 0.95**e)\n",
    "]\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_split=0.2, callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02eb257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_benchmark_vwap(daily_df):\n",
    "    if daily_df.empty or daily_df['volume'].sum() == 0:\n",
    "        return np.nan\n",
    "    return (daily_df['avg_price'] * daily_df['volume']).sum() / daily_df['volume'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83e37ed",
   "metadata": {},
   "source": [
    "### Evaluation Function\n",
    "\n",
    "Defines the backtesting function, grouping data by date and simulating trades using historical data only for volatility and volume calculations to avoid bias. It predicts schedules, applies market impact, accumulates trade values, and computes slippage metrics. The function returns a DataFrame of daily results for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde7eeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tln_strategy(test_df, model, total_shares_to_trade=1_000_000):\n",
    "    daily_groups = test_df.groupby(test_df.index.date)\n",
    "    results = []\n",
    "\n",
    "    for date, day_data in daily_groups:\n",
    "        if len(day_data) < LOOKBACK + HORIZON:\n",
    "            continue\n",
    "\n",
    "        benchmark_vwap = calculate_benchmark_vwap(day_data)\n",
    "        if pd.isna(benchmark_vwap):\n",
    "            continue\n",
    "\n",
    "        total_value_of_trades = 0\n",
    "        total_shares_traded = 0\n",
    "\n",
    "        for i in range(len(day_data) - LOOKBACK):\n",
    "            historical_data = day_data.iloc[:i + LOOKBACK + 1]\n",
    "            daily_volatility = historical_data['avg_price'].std() if len(historical_data) > 1 else 0.0\n",
    "            avg_daily_volume = historical_data['volume'].mean() if len(historical_data) > 0 else 1.0\n",
    "\n",
    "            input_sequence = historical_data.iloc[-LOOKBACK:][FEATURES].values\n",
    "            input_sequence = np.expand_dims(input_sequence, axis=0)\n",
    "\n",
    "            predicted_schedule = model.predict(input_sequence, verbose=0)[0]\n",
    "\n",
    "            shares_to_trade = predicted_schedule[0] * total_shares_to_trade\n",
    "            base_price = day_data.iloc[i + LOOKBACK]['avg_price']\n",
    "\n",
    "            impact = daily_volatility * np.sqrt(shares_to_trade / avg_daily_volume) if avg_daily_volume > 0 else 0.0\n",
    "            execution_price = base_price + impact\n",
    "\n",
    "            total_value_of_trades += shares_to_trade * execution_price\n",
    "            total_shares_traded += shares_to_trade\n",
    "\n",
    "        if total_shares_traded == 0:\n",
    "            continue\n",
    "\n",
    "        model_vwap = total_value_of_trades / total_shares_traded\n",
    "        slippage = model_vwap - benchmark_vwap\n",
    "        slippage_bps = (slippage / benchmark_vwap) * 10000 if benchmark_vwap != 0 else np.nan\n",
    "\n",
    "        results.append({\n",
    "            'date': date.strftime('%Y-%m-%d'),\n",
    "            'benchmark_vwap': benchmark_vwap,\n",
    "            'model_vwap': model_vwap,\n",
    "            'slippage': slippage,\n",
    "            'slippage_bps': slippage_bps\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9e9780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive metrics\n",
    "def calculate_metrics(results_df):\n",
    "    metrics = {}\n",
    "    slippage_bps = results_df['slippage_bps'].dropna()\n",
    "    if len(slippage_bps) == 0:\n",
    "        return metrics\n",
    "\n",
    "    metrics['avg_slippage_bps'] = slippage_bps.mean()\n",
    "    metrics['std_slippage_bps'] = slippage_bps.std()\n",
    "    metrics['win_rate'] = (slippage_bps < 0).mean() * 100\n",
    "    metrics['sharpe_ratio'] = -metrics['avg_slippage_bps'] / metrics['std_slippage_bps'] if metrics['std_slippage_bps'] != 0 else 0\n",
    "    metrics['max_drawdown'] = (slippage_bps.cumsum().cummax() - slippage_bps.cumsum()).max()\n",
    "    metrics['mae'] = mean_absolute_error(results_df['benchmark_vwap'], results_df['model_vwap'])\n",
    "    metrics['rmse'] = np.sqrt(mean_squared_error(results_df['benchmark_vwap'], results_df['model_vwap']))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfbf7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "results_df = evaluate_tln_strategy(test_df, model)\n",
    "metrics = calculate_metrics(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41625e79",
   "metadata": {},
   "source": [
    "### Slippage Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56660f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(results_df['slippage_bps'].dropna(), bins=30, edgecolor='black')\n",
    "plt.title('Slippage Distribution (bps)')\n",
    "plt.xlabel('Slippage (bps)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
